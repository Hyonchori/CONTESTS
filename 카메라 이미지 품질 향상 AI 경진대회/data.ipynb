{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc4579e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "DATA_DIR = \"235746_카메라_품질_향상_AI경진대회_data/\"\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, isTrain, transform=None):\n",
    "        self.root = root\n",
    "        self.isTrain = isTrain\n",
    "        self.transform = transform\n",
    "        self.imgs_pd = pd.read_csv(self.root + \"train.csv\")\n",
    "        self.imgs_path = self.root + \"train_input_img/\" + self.imgs_pd[\"input_img\"].values\n",
    "        self.labels_path = self.root + \"train_label_img/\" + self.imgs_pd[\"label_img\"].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_path)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        origin = np.array(Image.open(self.imgs_path[idx]).convert(\"RGB\"))\n",
    "        img = Image.open(self.imgs_path[idx]).convert(\"RGB\")\n",
    "        label = Image.open(self.labels_path[idx]).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            label = self.transform(label)\n",
    "            \n",
    "        if self.isTrain:\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(\n",
    "                img, output_size=(612, 816))\n",
    "            img = TF.crop(img, i, j, h, w)\n",
    "            label = TF.crop(label, i, j, h, w)\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4117aa9",
   "metadata": {},
   "source": [
    ">**사진크기가 다른 경우가 있음**\n",
    ">- transform 으로 사진 크기를 일정하게 맞추기\n",
    ">- 작은 크기를 기준으로 사진이 크면 그 크기에 맞춰서 cropping??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bbc208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "valid_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = ImageDataset(DATA_DIR, True, train_transforms)\n",
    "valid_dataset = ImageDataset(DATA_DIR, False, valid_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82a77452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(522, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.arange(0, len(train_dataset))\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, indices[: -100])\n",
    "valid_dataset = torch.utils.data.Subset(valid_dataset, indices[-100: ])\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb287c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826dc8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2002f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNet.from_pretrained('efficientnet-b0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d52ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2f17f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1224, 1632])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a630525613b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "for img, label in valid_dataloader:\n",
    "    print(img.size())\n",
    "    pred = model(img)\n",
    "    print(pred.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a665df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from effunet import EffUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96faf11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    }
   ],
   "source": [
    "model = EffUNet(model=\"b4\", \n",
    "                out_channels=1, \n",
    "               freeze_backbone=True,\n",
    "               pretrained=True,\n",
    "               device=\"cpu\",\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d778a29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 612, 816])\n",
      "True\n",
      "torch.Size([1, 3, 448, 448])\n",
      "torch.Size([1, 1, 448, 448])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ori' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-fd811e61d125>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mori\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ori' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for img, label in train_dataloader:\n",
    "    #print(ori.size())\n",
    "    print(img.size())\n",
    "    print(img.size() == (1, 3, 612, 816))\n",
    "    tmp = F.interpolate(img, size=448)\n",
    "    print(tmp.size())\n",
    "    pred = model(tmp)\n",
    "    print(pred.size())\n",
    "    plt.imshow(ori[0])\n",
    "    plt.show()\n",
    "    plt.imshow(img[0].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.imshow(label[0].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.imshow(tmp[0].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.imshow(pred[0].permute(1, 2, 0).detach().numpy())\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd661454",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e008c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
